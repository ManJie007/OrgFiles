#+title: kubernetes_in_action
#+author: JIE MAN
* “Chapter 1. Introducing Kubernetes”
** Moving from monolithic apps to microservices

[[./pictures/kubernetes/1.png]]

Microservices communicate through synchronous protocols such as HTTP, over which they usually expose RESTful (REpresentational State Transfer) APIs, or through asynchronous protocols such as AMQP (Advanced Message Queueing Protocol).

Scaling microservices
只需要扩展那些真正需要更多资源的服务，而其他服务可以保持原有的规模不变。

[[./pictures/kubernetes/2.png]]

Microservices also bring other problems, such as making it hard to debug and trace execution calls, because they span multiple processes and machines. 
Luckily, these problems are now being addressed with distributed tracing systems such as =Zipkin=.
** Kubernetes uses Linux container technologies to provide isolation of running applications

[[./pictures/kubernetes/3.png]]
Using VMs to isolate groups of applications vs. isolating individual apps with containers
[[./pictures/kubernetes/4.png]]
The difference between how apps in VMs use the CPU vs. how they use them in containers

=Introducing the mechanisms that make container isolation possible=
=1.Linux Namespaces=
  makes sure each process sees its own personal view of the system (files, processes, network interfaces, hostname, and so on).

  The following kinds of namespaces exist:
    Mount (mnt)
    Process ID (pid)
    Network (net)
    Inter-process communication (ipc)
    UTS
    User Id(user)
      
=2.Linux Control Groups (cgroups)=
  limit the amount of resources the process can consume (CPU, memory, network bandwidth, and so on).
** Introducing the Docker container platform

=A big difference between Docker-based container images and VM images is that container images are composed of layers, which can be shared and reused across multiple images.=

Docker is a platform for packaging, distributing, and running applications.

main concepts:Images(contains the file system and other metadata), Registries, Containers
[[./pictures/kubernetes/5.png]]

[[./pictures/kubernetes/6.png]]
Running six apps on three VMs vs. running them in Docker containers
=Understanding image layers=
  Docker images are composed of layers. 
  Different images can contain the exact same layers because every Docker image is built on top of another image and two different images can both use the same parent image as their base.
  Two containers created from two images based on the same base layers can therefore read the same files,
  =but if one of them writes over those files, the other one doesn’t see those changes.=
  =This works because container image layers are read-only.=

  When a container is run,
    =a new writable layer is created on top of the layers in the image.=
  When the process in the container writes to a file located in one of the underlying layers,
    =a copy of the whole file is created in the top-most layer and the process writes to the copy.=

=You can’t containerize an application built for the x86 architecture and expect it to run on an ARM-based machine because it also runs Docker.=
** Introducing rkt—an alternative to Docker
rkt is a platform for running containers.

It uses the OCI container image format
and can even run regular Docker container images.
** Introducing Kubernetes
[[./pictures/kubernetes/7.png]]
The system is composed of a master node and any number of worker nodes.
When the developer submits a list of apps to the master, Kubernetes deploys them to the cluster of worker nodes. 
This includes things such as service discovery, scaling, load-balancing, self-healing, and even leader election.

[[./pictures/kubernetes/8.png]]
The components that make up a Kubernetes cluster
  The Control Plane
    The Kubernetes API Server, which you and the other Control Plane components communicate with
    The Scheduler, which schedules your apps (assigns a worker node to each deployable component of your application)
    The Controller Manager, which performs cluster-level functions, such as replicating components, keeping track of worker nodes, handling node failures, and so on
    etcd, a reliable distributed data store that persistently stores the cluster configuration.
  The nodes
    Docker, rkt, or another container runtime, which runs your containers
    The Kubelet, which talks to the API server and manages containers on its node
    The Kubernetes Service Proxy (kube-proxy), which load-balances network traffic between application components
** Running an application in Kubernetes
1.package it up into one or more container images, push those images to an image registry
2.post a description of your app to the Kubernetes API server
  The description includes information:
    1.the container image or images that contain your application components
    2.how those components are related to each other
    3.which ones need to be run co-located (together on the same node) and which don’t.
    4.For each component, you can also specify how many copies (or replicas) you want to run.
    5.which of those components provide a service to either internal or external clients
    6.should be exposed through a single IP address and made discoverable to the other components
3.When the API server processes your app’s description, the Scheduler schedules the specified groups of containers onto the available worker nodes based on computational resources required by each group and the unallocated resources on each node at that moment.
4.The Kubelet on those nodes then instructs the Container Runtime (Docker, for example) to pull the required container images and run the containers.
5.Once the application is running, Kubernetes continuously makes sure that the deployed state of the application always matches the description you provided.

[[./pictures/kubernetes/9.png]]

=Scaling the number of copies=
  While the application is running, you can decide you want to increase or decrease the number of copies, and Kubernetes will spin up additional ones or stop the excess ones, respectively.

=Hitting a moving target=
  If the container is providing a service to external clients or other containers running in the cluster, how can they use the container properly if it’s constantly moving around the cluster?
  And how can clients connect to containers providing a service when those containers are replicated and spread across the whole cluster?

  =To allow clients to easily find containers that provide a specific service, you can tell Kubernetes which containers provide the same service and Kubernetes will expose all of them at a single static IP address and expose that address to all applications running in the cluster.=
  =This is done through environment variables, but clients can also look up the service IP through good old DNS.=
  =The kube-proxy will make sure connections to the service are load balanced across all the containers that provide the service.=
  =The IP address of the service stays constant, so clients can always connect to its containers, even when they’re moved around the cluster.=

developers don’t need to implement features that they would usually implement.
This includes discovery of services and/or peers in a clustered application.
Kubernetes does this instead of the app.
Usually, the app only needs to =look up certain environment variables or perform a DNS lookup.=
If that’s not enough, the application can =query the Kubernetes API server directly= to get that and/or other information.
* “Chapter 2. First steps with Docker and Kubernetes”
** Creating, running, and sharing a container image with Docker
If you’re using a Mac or Windows and install Docker per instructions,
Docker will set up a VM for you and run the Docker daemon inside that VM.
The Docker client executable will be available on your host OS, and will communicate with the daemon inside the VM.

http://docs.docker.com/engine/installation/

busybox image
~docker pull busybox:tag~
~docker run busybox echo "Hello world"~

[[./pictures/kubernetes/10.png]]

Docker Hub registry
http://docker.io/
http://hub.docker.com

~docker run <image>:<tag>~

Creating a trivial Node.js app
=you’ll see that an app running inside a container sees its own hostname and not that of the host machine,=
even though it’s running on the host like any other process.
This will be useful later,
=when you deploy the app on Kubernetes and scale it out (scale it horizontally; that is, run multiple instances of the app).=

app.js
#+begin_src js
const http = require('http');
const os = require('os');

console.log("Kubia server starting...");

var handler = function(request, response) {
  console.log("Received request from " + request.connection.remoteAddress);
  response.writeHead(200);
  response.end("You've hit " + os.hostname() + "\n");
};

var www = http.createServer(handler);
www.listen(8080);
#+end_src
*** Creating a Dockerfile for the image

Dockerfile
#+begin_src Dockerfile
FROM node:7
ADD app.js /app.js
ENTRYPOINT ["node", "app.js"]
#+end_src

The Dockerfile needs to be in the same directory as the app.js file
*** Building the container image

~docker build -t kubia .~

自己之前使用的时候 用过 --platform

[[./pictures/kubernetes/11.png]]
*** Understanding how an image is built
The contents of the whole directory are uploaded to the Docker daemon and the image is built there.

=Understanding image layers=
An image isn’t a single, big, binary blob, =but is composed of multiple layers=
=Different images may share several layers, which makes storing and transferring images much more efficient.=
=When building an image, a new layer is created for each individual command in the Dockerfile.=

[[./pictures/kubernetes/12.png]]

~docker images~
*** Running the container image
~docker run --name kubia-container -p 8080:8080 -d kubia~

-d detached from the console
-p port mapped
自己之前使用的时候 用过 --platform

Docker 的 client 和 Dameon 可以不在一个主机
随意使用时需要指明Dameon所在的主机
可以在命令行中指明 或 DOCKER_HOST 环境变量

~curl localhost:8080~
*** Listing all running containers

~docker ps~
~docker ps -a~

Getting additional information about a container
~docker inspect kubia-container~

~docker exec -it kubia-container bash~
-i, which makes sure STDIN is kept open. You need this for entering commands into the shell.
-t, which allocates a pseudo terminal (TTY).
=exit= 退出

=The container’s filesystem is also isolated=
*** Stopping and removing a container
~docker stop kubia-container~

~docker start kubia-container~
~docker start -ai kubia-container~
-a	（attach）附加到容器的输出，你可以实时看到标准输出 / 错误
-i	（interactive）保持输入流打开，可进行交互输入（如 shell）

~docker ps -a~

~docker rm kubia-container~
*** Pushing the image to an image registry
http://hub.docker.com

Other widely used such registries are Quay.io and the Google Container Registry.

=Docker Hub will allow you to push an image if the image’s repository name starts with your Docker Hub ID.=
Tagging an image under an additional tag
  ~docker tag kubia luksa/kubia~
  ~docker images~

~docker login~
~docke push luksa/kubia~
*** Setting up a Kubernetes cluster
http://kubernetes.io
Kubernetes can be run on your local development machine, your own organization’s cluster of machines, on cloud providers providing virtual machines (Google Compute Engine, Amazon EC2, Microsoft Azure, and so on), or by using a managed Kubernetes cluster such as Google Kubernetes Engine (previously known as Google Container Engine).
=kubeadm tool=
=kops tool=
  install Kubernetes on Amazon’s AWS (Amazon Web Services)
  http://github.com/kubernetes/kops
  is built on top of kubeadm

=Minikube=  
  The simplest and quickest path to a fully functioning Kubernetes cluster is by using Minikube.
  http://github.com/kubernetes/minikube
~brew install minikube~
or
~curl -LO https://github.com/kubernetes/minikube/releases/latest/download/minikube-darwin-arm64~
~sudo install minikube-darwin-arm64 /usr/local/bin/minikube~
*** Starting a Kubernetes cluster with Minikube
~minikube start~
*** Installing the Kubernetes client (kubectl)
~brew install kubectl~
or
~curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl"~
~curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl.sha256"~
~echo "$(cat kubectl.sha256)  kubectl" | shasum -a 256 --check~
~chmod +x ./kubectl~
~sudo mv ./kubectl /usr/local/bin/kubectl~
~sudo chown root: /usr/local/bin/kubectl~

~kubectl version --client~
*** Checking to see the cluster is up and kubectl can talk to it
~kubectl cluster-info~

~minikube ssh~ to log into the Minikube VM and explore it from the inside.
*** Using a hosted Kubernetes cluster with Google Kubernetes Engine
use a managed Google Kubernetes Engine (GKE) cluster.

https://cloud.google.com/container-engine/docs/before-you-begin

1.Signing up for a Google account, in the unlikely case you don’t have one already.
2.Creating a project in the Google Cloud Platform Console.
3.Enabling billing. This does require your credit card info, but Google provides a 12-month free trial. And they’re nice enough to not start charging automatically after the free trial is over.)
4.Enabling the Kubernetes Engine API.
5.Downloading and installing Google Cloud SDK. (This includes the gcloud command-line tool, which you’ll need to create a Kubernetes cluster.)
6.Installing the kubectl command-line tool with gcloud components install kubectl.

Creating a Kubernetes cluster with three nodes
~gcloud container clusters create kubia --num-nodes 3 --machine-type f1-micro~
[[./pictures/kubernetes/13.png]]
*** Checking if the cluster is up by listing cluster nodes
~kubeclt get nodes~

Google Kubernetes Engine (GKE) cluster
~gcloud compute ssh <node-name>~
*** Retrieving additional details of an object
~kubectl describe node <node-name>~
*** Setting up an alias and command-line completion for kubectl
~/.bashrc or ~/.zshrc
#+begin_src shell
alias k=kubectl
#+end_src

tab completion in bash
install a package called /bash-completion/
~source <(kubectl completion bash)~
~source <(kubectl completion bash | sed s/kubectl/k/g)~
*** Running your first app on Kubernetes
=Usually, you’d prepare a JSON or YAML manifest, containing a description of all the components you want to deploy=

这里先使用简单的命令

~kubectl run kubia --image=luksa/kubia --port=8080 --generator=run/v1~
--generator
  Usually, you won’t use it
  =using it here so Kubernetes creates a ReplicationController instead of a Deployment.=
  a ReplicationController called kubia has been created.
  后面介绍
*** Pods!!!
=A pod is a group of one or more tightly related containers that will always run together on the same worker node and in the same Linux namespace(s).=
each pod has its own IP
[[./pictures/kubernetes/14.png]]

~kubectl get pods~
~kubectl describe pod~

[[./pictures/kubernetes/15.png]]
*** Accessing
=With your pod running, how do you access it?=
  =each pod gets its own IP address, but this address is internal to the cluster and isn’t accessible from outside of it.=
  =To make the pod accessible from the outside, you’ll expose it through a Service object.=
  You’ll create a special service of type =LoadBalancer=
    because if you create a =regular service (a ClusterIP service)=, like the pod, it would also only be accessible from inside the cluster.

~kubectl expose rc kubia --type=LoadBalancer --name kubia-http~
  the abbreviation rc instead of replicationcontroller
  =po for pods, svc for services, and so on=

=Services are objects like Pods and Nodes,=
  so you can see the newly created Service object by running the kubectl get services command
  ~kubectl get services~
  ~kubectl get svc~
  可以看到可以访问服务的外部IP

Minikube doesn’t support LoadBalancer services, so the service will never get an external IP.
But you can access the service anyway through its external port.
When using Minikube, you can get the IP and port through which you can access the service by running
~minikube service kubia-http~
*** Understanding how the ReplicationController, the Pod, and the Ser- rvice fit together
[[./pictures/kubernetes/16.png]]

Generally, ReplicationControllers are used to replicate pods (that is, create multiple copies of a pod) and keep them running.

=A pod may disappear at any time=
  because the node it’s running on has failed, because someone deleted the pod, or because the pod was evicted from an otherwise healthy node.
  When any of those occurs, a missing pod is replaced with a new one by the Replication-Controller, as described previously.
  This new pod gets a different IP address from the pod it’s replacing.
  =This is where services come in—to solve the problem of ever-changing pod IP addresses, as well as exposing multiple pods at a single constant IP and port pair.=

=When a service is created, it gets a static IP, which never changes during the lifetime of the service.=
=Services represent a static location for a group of one or more pods that all provide the same service.=
=Requests coming to the IP and port of the service will be forwarded to the IP and port of one of the pods belonging to the service at that moment.=
*** Horizontally scaling the application
~kubectl get replicationcontrollers~
~kubectl scale rc kubia --replicas=3~
~kubectl get replicationcontrollers~
~kubectl get pods~

[[./pictures/kubernetes/17.png]]

~kubectl get pods -o wide~
~kubectl describe pod <pod-name>~
*** Introducing the Kubernetes dashboard
[[./pictures/kubernetes/18.png]]

~kubectl cluster-info | grep dashboard~

If you’re using Google Kubernetes Engine
  ~gcloud container clusters describe kubia | grep -E "(username|password):"~

~minikube dashboard~  
* “Chapter 3. Pods: running containers in Kubernetes”
** Introducing pods
A pod never spans two nodes.

[[./pictures/kubernetes/19.png]]

magine an app consisting of multiple processes that either communicate through IPC (Inter-Process Communication) or through locally stored files, which requires them to run on the same machine.
If you run multiple unrelated processes in a single container, it is your responsibility to keep all those processes running, manage their logs, and so on.
=Therefore, you need to run each process in its own container.=

=Because you’re not supposed to group multiple processes into a single container, it’s obvious you need another higher-level construct that will allow you to bind containers together and manage them as a single unit.=
--- =pods=
Kubernetes achieves this by configuring Docker to have all containers of a pod share the same set of Linux namespaces instead of each container having its own set.
all containers of a pod run under the =same Network and UTS namespaces (we’re talking about Linux namespaces here), they all share the same hostname and network interfaces.=
all containers of a pod run under the =same IPC namespace and can communicate through IPC.=
=also share the same PID namespace, but that feature isn’t enabled by default.=
Because most of the container’s filesystem comes from the container image, by default, the filesystem of each container is fully isolated from other containers.
=However, it’s possible to have them share file directories using a Kubernetes concept called a Volume=

=because containers in a pod run in the same Network namespace, they share the same IP address and port space.=
=This means processes running in containers of the same pod need to take care not to bind to the same port numbers or they’ll run into port conflicts.=
=All the containers in a pod also have the same loopback network interface, so a container can communicate with other containers in the same pod through localhost.=

=All pods in a Kubernetes cluster reside in a single flat, shared, network-address space=
every pod can access every other pod at the other pod’s IP address.
No NAT (Network Address Translation) gateways exist between them.
[[./pictures/kubernetes/20.png]]

Splitting the pod into two would allow Kubernetes to schedule the frontend to one node and the backend to the other node, thereby improving the utilization of your infrastructure.

=A pod is also the basic unit of scaling.!!!= 注意这里说 pod 是 缩放的基本单位
  If your pod consists of a frontend and a backend container, when you scale up the number of instances of the pod to, let’s say, two, you end up with two frontend containers and two backend containers.
  =Usually, frontend components have completely different scaling requirements than the backends, so we tend to scale them individually.=

=Pods should contain tightly coupled containers, usually a main container and containers that support the main one.=
[[./pictures/kubernetes/21.png]]
=sidecar container=
Other examples of sidecar containers include log rotators and collectors, data processors, communication adapters, and others.

A container shouldn’t run multiple processes.
A pod shouldn’t contain multiple containers if they don’t need to run on the same machine.
[[./pictures/kubernetes/22.png]]
** Creating pods from YAML or JSON descriptors
=Pods and other Kubernetes resources are usually created by posting a JSON or YAML manifest to the Kubernetes REST API endpoint.=
https://kubernetes.io/docs/reference/
~kubectl get -o yaml~

=Kubernetes 中看到的 Pod、Deployment、Service、ConfigMap、Namespace、Secret、Node、Volume 等，统统都是一种 API 对象。=
=这些对象都是以结构化数据（通常是 YAML 或 JSON）形式定义，并通过 Kubernetes API Server 管理的。=

Creating a simple YAML descriptor for a pod
#+begin_src yaml
apiVersion: v1
kind: Pod
metadata: 
  name: kubia-manaul
spec:
  containers:
  - image: luksa/kubia
  name: kubia
  ports:
  - containerPort: 8080
    protocol: TCP
#+end_src

即使你在 Pod 的定义中没有明确写出端口，只要容器本身在监听某个端口（比如绑定在 0.0.0.0 上），其他 Pod 依然可以通过这个端口访问它，不会因此被限制。
虽然不写 ports 不影响通信，但出于可读性、可维护性以及后续功能的便利性，显式声明端口是推荐做法。
** Using kubectl explain to discover possible API object fields
~kubectl explain pods~

http://kubernetes.io/docs/api

~kubectl explain pod.spec~
** Using kubectl create to create the pod
~kubectl create -f kubia-manual.yaml~

~kubectl get pod kubia-manual -o yaml~

~kubectl get pod kubia-manual -o json~

~kubectl get pods~
** Viewing application logs
=Containerized applications usually log to the standard output and standard error stream instead of writing their logs to files.=

The container runtime (Docker in your case) redirects those streams to files and allows you to get the container’s log by running
~docker logs <container-id>~

=could use ssh to log into the node where your pod is running and retrieve its logs with docker logs,=
but Kubernetes provides an easier way.
** Retrieving a pod’s log with kubectl logs
To see your pod’s log
~kubectl logs kubia-manual~

Container logs are automatically rotated daily and every time the log file reaches 10MB in size.
The kubectl logs command only shows the log entries from the last rotation.

~kubectl logs kubia-manual -c kubia~

=To make a pod’s logs available even after the pod is deleted, you need to set up centralized, cluster-wide logging, which stores all the logs into a central store.=
=Chapter 17 explains how centralized logging works.=
** Sending requests to the pod
~kubectl~ expose command to create a service to gain access to the pod externally.

=port forwarding=
  other ways of connecting to a pod for testing and debugging purposes.
  ~kubectl port-forward kubia-manual 8888:8888~
  ~curl localhost:8080~

  [[./pictures/kubernetes/23.png]]
** Organizing pods with labels
=Organizing pods and all other Kubernetes objects is done through labels.=

[[./pictures/kubernetes/24.png]]

=A canary release is when you deploy a new version of an application next to the stable version, and only let a small fraction of users hit the new version to see how it behaves before rolling it out to all users.=
=This prevents bad releases from being exposed to too many users.=

Specifying labels when creating a pod
kubia-manual-with-labels.yaml
#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual-v2
  labels:
    creation_method: manual 
    env: prod 
spec:
  containers:
    - image: luksa/kubia
  name: kubia
  ports:
    - containerPort: 8080
      protocol: TCP
#+end_src

~kubectl create -f kubia-manual-with-labels.yaml~
~kubectl get pods --show-labels~
~kubectl get pods -L creation_methon,enc~
** Modifying labels of existing pods
~kubectl label pods kubia-manual creation_method=manual~

~kubectl label pods kubia-manual-v2 env=debug --overwrite~

~kubectl get pods -L creation_method,env~
** Listing subsets of pods through label selectors
~kubectl get pods -l creation_method=manual~

~kubectl get pods -l env~

~kubectl get pods -l '!env'~

~kubectl get pods -l creation_method!=manual~

Selecting the product catalog microservice pods using the “app=pc” label selector
[[./pictures/kubernetes/25.png]]

~kubectl get pods -l app=pc,rel=beta~
[[./pictures/kubernetes/26.png]]
** Using labels and selectors to constrain pod scheduling
All the pods you’ve created so far have been scheduled pretty much randomly across your worker nodes.

Example is when you need to schedule pods performing intensive GPU-based computation only to nodes that provide the required GPU acceleration.

=Using labels for categorizing worker nodes=
=Labels can be attached to any Kubernetes object, including nodes.=
~kubectl label node gke-kubia-85f6-node-0rrx gpu=true~
~kubectl get nodes -l gpu=true~
~kubectl get nodes -L gpu=true~
列出所有节点，并在结果中显示它们是否有 gpu=true 标签（添加标签列显示）
** Scheduling pods to specific nodes
~kubectl create -f kubia-gpu.yaml~

kubia-gpu.yaml
#+begin_src yaml
apiVersion: v1
kind: Pod
metadata
  name: kubia-gpu
spec:
  nodeSelector:
    gpu: "true"
  containers:
  - image: luksa/kubia
  name: kubia
#+end_src

=nodeSelector=

=you could also schedule a pod to an exact node, because each node also has a unique label with the=
=key :kubernetes.io/hostname, value: actual hostname of the node=
=setting the nodeSelector to a specific node by the hostname label may lead to the pod being unschedulable if the node is offline.=
** Annotating pods
=In addition to labels, pods and other objects can also contain annotations.=

=Annotations are used instead of fields, and then once the required API changes have become clear and been agreed upon by the Kubernetes developers, new fields are introduced and the related annotations deprecated.=

=A great use of annotations is adding descriptions for each pod or other API object, so that everyone using the cluster can quickly look up information about each individual object.=

~kubectl get pods kubia-zxzij -o yaml~
  the kubernetes.io/created-by annotation holds JSON data about the object that created the pod.

~kubectl annotate pod kubia-manual mycompany.com/someannotation="foo bar"~
~kubeclt describe pod kubia-manual~
** Using namespaces to group resources

Kubernetes namespaces provide a scope for objects names.
=which also allows you to use the same resource names multiple times (across different namespaces)=

can also be used for separating resources in a multi-tenant environment, splitting up resources into production, development, and QA environments, or in any other way you may need

但有一种 K8s API Object -- Node resource, which is global and not tied to a single namespace.

=Kubernetes 中看到的 Pod、Deployment、Service、ConfigMap、Namespace、Secret、Node、Volume 等，统统都是一种 API 对象。=
=这些对象都是以结构化数据（通常是 YAML 或 JSON）形式定义，并通过 Kubernetes API Server 管理的。=

 ~kubectl get ns~
 ~kubectl get pod --namespace kube-system~
 ➜  OrgFiles git:(main) ✗ kubectl get pods --namespace kube-system
NAME                               READY   STATUS    RESTARTS      AGE
coredns-668d6bf9bc-wjftr           1/1     Running   4 (31s ago)   11d
etcd-minikube                      1/1     Running   4 (36s ago)   11d
kube-apiserver-minikube            1/1     Running   4 (26s ago)   11d
kube-controller-manager-minikube   1/1     Running   5 (36s ago)   11d
kube-proxy-gt5hb                   1/1     Running   4 (36s ago)   11d
kube-scheduler-minikube            1/1     Running   4 (36s ago)   11d
metrics-server-7fbb699795-zbbps    1/1     Running   7 (19s ago)   11d
storage-provisioner                1/1     Running   8 (23s ago)   11d

=Besides isolating resources, namespaces are also used for allowing only certain users access to particular resources and even for limiting the amount of computational resources available to individual users.=

A namespace is a Kubernetes resource like any other, so you can create it by posting a YAML file to the Kubernetes API server.
=Kubernetes 中看到的 Pod、Deployment、Service、ConfigMap、Namespace、Secret、Node、Volume 等，统统都是一种 API 对象。=
=这些对象都是以结构化数据（通常是 YAML 或 JSON）形式定义，并通过 Kubernetes API Server 管理的。=

custom-namespace.yaml
#+begin_src yaml
apiVersion: v1
kind: Namespace
metadata:
  name : custom-namespace  
#+end_src

~kubectl create -f custom-namespace.yaml~
~kubectl create namepsace custom-namespace~

=everything in Kubernetes has a corresponding API object that you can create, read, update, and delete by posting a YAML manifest to the API server=
=Kubernetes 中看到的 Pod、Deployment、Service、ConfigMap、Namespace、Secret、Node、Volume 等，统统都是一种 API 对象。=
=这些对象都是以结构化数据（通常是 YAML 或 JSON）形式定义，并通过 Kubernetes API Server 管理的。=

Managing objects in other namespaces
~kubectl create -f kubia-manual.yaml -n custom-namespace~
=When listing, describing, modifying, or deleting objects in other namespaces, you need to pass the --namespace (or -n) flag to kubectl.=

The current context’s namespace and the current context itself can be changed through =kubectl config commands.=

为了快速输入命令
~/.xxxrc
#+begin_src shell
alias kcb='kubectl config set-context $(kubectl config current-context) --namespace'
#+end_src

=Whether namespaces provide network isolation depends on which networking solution is deployed with Kubernetes.=
** Stopping and removing pods
delete by name
~kubectl delete pod kubia-gpu~

=Kubernetes sends a SIGTERM signal to the process and waits a certain number of seconds (30 by default) for it to shut down gracefully.=
=If it doesn’t shut down in time, the process is then killed through SIGKILL.=
=To make sure your processes are always shut down gracefully, they need to handle the SIGTERM signal properly.=

~kubectl delete po pod1 pod2~

delete pods using label selectors
~kubectl delete po -l creating_method=manual~
~kubectl delete po -l rel=cancary~

canary 灰度发布/小流量发布

[[./pictures/kubernetes/27.png]]

delete by namespace
~kubectl delete ns custom-namespace~

delete all pods in a namespace, while keeping the namespace
~kubectl get pods~
delete all pods in the current namespace by using the --all option
~kubectl delete pod --all~
~kubectl get pods~
Wait, what!?! The kubia-zxzij pod is terminating, but a new pod called kubia-09as0, which wasn’t there before, has appeared. No matter how many times you delete all pods, a new pod called kubia-something will emerge.
~kubectl run~ =this doesn’t create a pod directly, but instead creates a ReplicationController, which then creates the pod.=
=As soon as you delete a pod created by the ReplicationController, it immediately creates a new one.=
=To delete the pod, you also need to delete the ReplicationController.=

delete almost all resources in a namespace
~kubectl delete all --all~

Deleting everything with the all keyword doesn’t delete absolutely everything.
=Certain resources (like Secrets, which we’ll introduce in chapter 7) are preserved and need to be deleted explicitly.=
* “Chapter 4. Replication and other controllers: deploying managed pods”
pods represent the basic deployable unit in Kubernetes.

never create pods directly
=create ReplicationControllers or Deployments to manage the actual pods=
** Keeping pods healthy
If the container’s main process crashes, the =Kubelet= will restart the container.

But sometimes apps stop working without their process crashing.
  =For example, a Java app with a memory leak will start throwing OutOfMemoryErrors, but the JVM process will keep running.=
  =It would be great to have a way for an app to signal to Kubernetes that it’s no longer functioning properly and have Kubernetes restart it.=

it still doesn’t solve all your problems.

For example, what about those situations when your app stops responding because it falls into an infinite loop or a deadlock?
To make sure applications are restarted in such cases, you must check an application’s health from the outside and not depend on the app doing it internally.

=Kubernetes can check if a container is still alive through liveness probes=

=can specify a liveness probe for each container in the pod’s specification.=
=Kubernetes will periodically execute the probe and restart the container if the probe fails.=

=Kubernetes also supports readiness probes=

Kubernetes can probe a container using one of the three mechanisms:
  An HTTP GET probe performs an HTTP GET request on the container’s IP address, a port and path you specify.
    If the probe receives a response, and the response code doesn’t represent an error (in other words, if the HTTP response code is 2xx or 3xx), the probe is considered successful.
    If the server returns an error response code or if it doesn’t respond at all, the probe is considered a failure and the container will be restarted as a result.
  A TCP Socket probe tries to open a TCP connection to the specified port of the container.
    If the connection is established successfully, the probe is successful.
    Otherwise, the container is restarted.
  An Exec probe executes an arbitrary command inside the container and checks the command’s exit status code.
    If the status code is 0, the probe is successful. All other codes are considered failures.

=Creating an HTTP-based liveness probe=
create a new pod that includes an HTTP GET liveness probe.

kubia-liveness-probe.yaml
#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy
  name: kubia
  livenessProbe:
    httpGet:
      path: /
      port: 8080  
#+end_src

~kubectl get pod kubia-liveness~

Obtaining the application log of a crashed container
~kubectl logs~
~kubectl logs mypod --pervious~

~kubectl describe po kubia-liveness~

~kubectl describe po kubia-liveness~

Configuring additional properties of the liveness probe
~kubectl describe~
=These additional parameters can be customized when defining the probe.=
add the initialDelaySeconds property to the liveness probe
#+begin_src yaml
livenessProbe
  httpGet:
    path: /
    port: 8080
  initialDelaySecounds: 15
#+end_src

=If you don’t set the initial delay, the prober will start probing the container as soon as it starts, which usually leads to the probe failing, because the app isn’t ready to start receiving requests.=
=If the number of failures exceeds the failure threshold, the container is restarted before it’s even able to start responding to requests properly.=
=Always remember to set an initial delay to account for your app’s startup time.=

~kubectl describe~
they’d have seen that the container terminated with exit code 137 or 143, telling them that the pod was terminated externally.
=Additionally, the listing of the pod’s events would show that the container was killed because of a failed liveness probe.=
Exit code 137 signals that the process was killed by an external signal (exit code is 128 + 9 (SIGKILL).
Likewise, exit code 143 corresponds to 128 + 15 (SIGTERM).

=For pods running in production, you should always define a liveness probe.=
=But for a better liveness check, you’d configure the probe to perform requests on a specific URL path (/health, for example) and have the app perform an internal status check of all the vital components running inside the app to ensure none of them has died or is unresponsive.=
=The probe’s CPU time is counted in the container’s CPU time quota, so having a heavyweight liveness probe will reduce the CPU time available to the main application processes.=

You’ve already seen that the failure threshold for the probe is configurable and usually the probe must fail multiple times before the container is killed.
=But even if you set the failure threshold to 1, Kubernetes will retry the probe several times before considering it a single failed attempt.=
=Therefore, implementing your own retry loop into the probe is wasted effort.=
This job is performed by the Kubelet on the node hosting the pod, the Kubernetes Control Plane components running on the master(s) have no part in this process.
Those pods aren’t managed by anything except by the Kubelet.
But if the node itself crashes, it’s the Control Plane that must create replacements for all the pods that went down with the node.
** ReplicationControllers
=A ReplicationController is a Kubernetes resource that ensures its pods are always kept running.=

[[./pictures/kubernetes/28.png]]

A ReplicationController constantly monitors the list of running pods and makes sure the actual number of pods of a “type” always matches the desired number.
  If too few such pods are running, it creates new replicas from a pod template.
  If too many such pods are running, it removes the excess replicas.
  how there can be more than the desired number of replicas. This can happen for a few reasons:
    Someone creates a pod of the same type manually.
    Someone changes an existing pod’s “type.”
    Someone decreases the desired number of pods, and so on.

I’ve used the term pod “type” a few times.
But no such thing exists.
=Replication-Controllers don’t operate on pod types, but on sets of pods that match a certain label selector=

A ReplicationController’s reconciliation loop
[[./pictures/kubernetes/29.png]]

A ReplicationController has three essential parts:
  A label selector, which determines what pods are in the ReplicationController’s scope
  A replica count, which specifies the desired number of pods that should be running
  A pod template, which is used when creating new pod replicas
[[./pictures/kubernetes/30.png]]

=Like pods and other Kubernetes resources, you create a ReplicationController by posting a JSON or YAML descriptor to the Kubernetes API server.=
kubia-rc.yaml
#+begin_src yaml
apiVersion: v1
kind: Replicationcontroller
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - namee: kubia
        images: luksa/kubia
        ports:
        - containerPort: 8080  
#+end_src

~kubectl create -f kubia-rc.yaml~

~kubectl get pods~

~kubectl delete pod XXX~
~kubectl get pods~
=the one deleted is terminating=

~kubectl get replicationcontroller/rc~
~kubectl describe rc kubia~
[[./pictures/kubernetes/31.png]]

using Google Kubernetes Engine to run these examples, you have a three-node Kubernetes cluster.
~gcloud compute ssh xxx~
~$sudo ifconfig eth0 down~

~kubectl get node~
~gcloud compute instance reset XXX~

=a ReplicationController manages pods that match its label selector.=
=By changing a pod’s labels, it can be removed from or added to the scope of a ReplicationController.=
=It can even be moved from one ReplicationController to another.=
=Although a pod isn’t tied to a ReplicationController, the pod does reference it in the metadata.ownerReferences field, which you can use to easily find which ReplicationController a pod belongs to.=
=keep in mind that when you changed the pod’s labels, the replication controller noticed one pod was missing and spun up a new pod to replace it.=

~kubectl label pod kubia-dmdck type=special~
~kubectl get pods --show-labels~
~kubectl label pod kubia=dmdck app=foo --overwrite~
~kubectl get pods -L app~
You’re using the -L app option to display the app label in a column.

Removing a pod from the scope of a ReplicationController by changing its labels
[[./pictures/kubernetes/32.png]]
If you know a pod is malfunctioning, you can take it out of the ReplicationController’s scope, let the controller replace it with a new one, and then debug or play with the pod in any way you want. Once you’re done, you delete the pod.

=Changing the ReplicationController’s label selector=
Kubernetes does allow you to change a ReplicationController’s label selector
You’ll never change a controller’s label selector, but you’ll regularly change its pod template.

=Changing the pod template=
Changing a ReplicationController’s pod template only affects pods created afterward and has no effect on existing pods.
[[./pictures/kubernetes/33.png]]

~kubectl edit rc kubia~
This will open the ReplicationController’s YAML definition in your default text editor.
After you save your changes and exit the editor, kubectl will update the ReplicationController
环境变量 $KUBE_EDITOR 选择对应的编辑器
~/.XXXrc

~export KUBE_EDITOR="emacs -nw"~
如果没有设置，则是 $EDITOR

=Horizontally scaling pods=
~kubectl scale rc kubia --replicas=10~
or
~kubectl edit rc kubia~
by editing the ReplicationController's definition
All this command does is modify the =spec.replicas= field of the ReplicationController’s definition

~kubectl get rc~

[[./pictures/kubernetes/34.png]]

When deleting a ReplicationController with kubectl delete, you can keep its pods running by passing the --cascade=false option to the command
~kubectl delete rc kubia --cascade=false~

=You’ve deleted the ReplicationController so the pods are on their own.=
They are no longer managed.
=But you can always create a new ReplicationController with the proper label selector and make them managed again.=
** Using ReplicaSets instead of ReplicationControllers

a new generation of ReplicationController and replaces it completely (ReplicationControllers will eventually be deprecated).

A ReplicaSet behaves exactly like a ReplicationController, but it has more expressive pod selectors.
For example, a ReplicaSet can match all pods that include a label with the key env, whatever its actual value is (you can think of it as env=*).

kuia-replicaset.yaml
#+begin_src yaml
apiVersion: apps/v1beta2
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        images: luska/kubia  
#+end_src

=API version attribute=
apiversion: apps/v1beta2
pecifies two things:
  • The API group (which is apps in this case)
  • The actual API version (v1beta2)

~kubectl get replicaset/rs~
~kubectl describe rs~

kubia=replicaset-match-experssions.yaml
#+begin_src yaml
...
  selector:
    matchExpression:
    - key: app
      operator: In
      values:
      - kubia
...  
#+end_src

each expression must contain a key, an operator, and possibly (depending on the operator) a list of values.

You’ll see four valid operators:
  In—Label’s value must match one of the specified values.
  NotIn—Label’s value must not match any of the specified values.
  Exists—Pod must include a label with the specified key (the value isn’t important). When using this operator, you shouldn’t specify the values field.
  DoesNotExist—Pod must not include a label with the specified key. The values property must not be specified.

Remember, always use them instead of ReplicationControllers, but you may still find ReplicationControllers in other people’s deployments.

~kubectl delete rs kubia~
=Deleting the ReplicaSet should delete all the pods.=
List the pods to confirm that’s the case.
** Running exactly one pod on each node with DaemonSets
Both ReplicationControllers and ReplicaSets are used for running a specific number of pods deployed anywhere in the Kubernetes cluster.

DaemonSets run only a single pod replica on each node, whereas ReplicaSets scatter them around the whole cluster randomly

[[./pictures/kubernetes/35.png]]

=Those cases include infrastructure-related pods that perform system-level operations.=
=For example, you’ll want to run a log collector and a resource monitor on every node.=
=Another good example is Kubernetes’ own kube-proxy process, which needs to run on all nodes to make services work.=
Outside of Kubernetes, such processes would usually be started through system init scripts or the systemd daemon during node boot up.
On Kubernetes nodes, you can still use systemd to run your system processes, but then you can’t take advantage of all the features Kubernetes provides.

If a node goes down, the DaemonSet doesn’t cause the pod to be created elsewhere.
But when a new node is added to the cluster, the DaemonSet immediately deploys a new pod instance to it.
Like a ReplicaSet, a DaemonSet creates the pod from the pod template configured in it.

=Using a DaemonSet to run pods only on certain nodes=
A DaemonSet deploys pods to all nodes in the cluster, unless you specify that the pods should only run on a subset of all the nodes.
This is done by specifying the =node-Selector property in the pod template=, which is part of the DaemonSet definition (similar to the pod template in a ReplicaSet or ReplicationController).

[[./pictures/kubernetes/36.png]]

used node selectors to deploy a pod onto specific nodes
  ~kubectl label node gke-kubia-85f6-node-0rrx gpu=true~
  
  kubia-gpu.yaml
  #+begin_src yaml
  apiVersion: v1
  kind: Pod
  metadata
    name: kubia-gpu
  spec:
    nodeSelector:
      gpu: "true"
    containers:
    - image: luksa/kubia
    name: kubia
  #+end_src
  
  ~kubectl create -f kubia-gpu.yaml~

Let’s imagine having a daemon called ssd-monitor that needs to run on all nodes that contain a solid-state drive (SSD).
  You’ll create a DaemonSet that runs this daemon on all nodes that are marked as having an SSD.
  The cluster administrators have added the disk=ssd label to all such nodes, so you’ll create the DaemonSet with a node selector that only selects nodes with that label.

Using a DaemonSet with a node selector to deploy system pods only on certain nodes
[[./pictures/kubernetes/36.png]]

ssd-monitor-daemon-ser.yaml
#+begin_src yaml
apiVersion: apps/v1beta2
kind: DaemonSet  
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor
#+end_src

~kubectl createe -f ssd-monitor-daemonset.yaml~
~kubectl get ds~
~kubectl get node~
~kubectl label node minikube disk=ssd~
~kubectl get po~

Removing the required label from the node
~kubectl label node minikube disk=hdd --overwrite~
~kubectl get po~
** Running pods that perform a single completable task
You’ll have cases where you only want to run a task that terminates after completing its work.

Processes in such pods are restarted when they exit.

But in a completable task, after its process terminates, it should not be restarted again.

=Kubernetes includes support for this through the Job resource, it allows you to run a pod whose container isn’t restarted when the process running inside finishes successfully.=

Pods managed by Jobs are rescheduled until they finish successfully.
In the event of a node failure, the pods on that node that are managed by a Job will be rescheduled to other nodes the way ReplicaSet pods are.
[[./pictures/kubernetes/37.png]]

exporter.yaml
#+begin_src yaml
apiVersion: batch/v1
kind: job
metadata:
  name: batch-job
spce:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: onFailure
      constainers:
      - name: main
        image: luksa/batch-job  
#+end_src

In a pod’s specification, you can specify what Kubernetes should do when the processes running in the container finish.
=This is done through the restartPolicy pod spec property, which defaults to Always.=
Job pods can’t use the default policy, because they’re not meant to run indefinitely.
=Therefore, you need to explicitly set the restart policy to either OnFailure or Never.=

~kubectl create -f exporter.yaml~
~kubectl get jobs~
~kubectl get pods~
~kubectl get pods --show-all~
~kubectl logs xxxPod~

=Running multiple pod instances in a Job=
Jobs may be configured to create more than one pod instance and run them in parallel or sequentially.
This is done by setting the completions and the parallelism properties in the Job spec.

#+begin_src yaml
apiVersion: batch/v1
kind: job
metadata:
  name: multi-completion-batch-job
spce:
  completions: 5
  parallelism: 2
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      constainers:
      - name: main
        image: luksa/batch-job  
#+end_src

~kubectl get pods~

scaling a Job, you also can

~kubectl scale job multi-completion-batch-job --replicas 3~

=Limiting the time allowed for a Job pod to complete=
=A pod’s time can be limited by setting the activeDeadlineSeconds property in the pod spec.=
=You can configure how many times a Job can be retried before it is marked as failed by specifying the spec.backoffLimit field in the Job manifest.=

=Scheduling Jobs to run periodically or once in the future=
many batch jobs need to be run at a specific time in the future or repeatedly in the specified interval.
In Linux- and UNIX-like operating systems, these jobs are better known as cron jobs.
Kubernetes supports them, too.

=Creating a CronJob=
cronkob.yaml
#+begin_src yaml
apiversion: batch/v1beta1
kind: CronJob
metadata:
  name: batch-job-every-fifteen-minutes
spec:
  schedule: "0,15,30,45 * * * *"
  jobTemplate:
    spce:
      template:
        metadata:
          labels:
            app: periodic-batch-job
        spec:
          restartpolicy: OnFailure
          containers:
          - name: main
            image: luska/batch-job
#+end_src

A CronJob creates Job resources from the jobTemplate property configured in the CronJob spec

You may have a hard requirement for the job to not be started too far over the scheduled time.
=In that case, you can specify a deadline by specifying the startingDeadlineSeconds field in the CronJob specification as shown in the following listing.=

cronkob.yaml
#+begin_src yaml
apiversion: batch/v1beta1
kind: CronJob
metadata:
  name: batch-job-every-fifteen-minutes
spec:
  schedule: "0,15,30,45 * * * *"
  startingDeadlineseconds: 15
  ...
#+end_src
* “Chapter 5. Services: enabling clients to discover and talk to pods”

Pods need a way of finding other pods if they want to consume the services they provide.

A Kubernetes Service is a resource you create to make a single, constant point of entry to a group of pods providing the same service.

Each service has an IP address and port that never change while the service exists.
The service address doesn’t change even if the pod’s IP address changes.
=Additionally, by creating the service, you also enable the frontend pods to easily find the backend service by its name through either environment variables or DNS.=

Both internal and external clients usually connect to pods through services.
[[./pictures/kubernetes/38.png]]

Connections to the service are load-balanced across all the backing pods.

You probably remember label selectors and how they’re used in Replication-Controllers and other pod controllers to specify which pods belong to the same set.
=The same mechanism is used by services in the same way=
=Label selectors determine which pods belong to the Service.=
[[./pictures/kubernetes/39.png]]

~kubectl expose~
  created a Service resource with the same pod selector as the one used by the ReplicationController,
  thereby exposing all its pods through a single IP address and port.
or
create a service manually by posting a =YAML= to the Kubernetes API server.

kubia-svc.yaml
#+begin_src yaml
apiVersion: v1
kind: service
metadata:
  name: kubia
spce:
  ports:
    - port: 80
      targetPort: 8080
    selector:
      app: kubia
#+end_src

~kubectl get services/svc~

=The primary purpose of services is exposing groups of pods to other pods in the cluster, but you’ll usually also want to expose services externally.=

~kubectl exec~ allows you to remotely run arbitrary commands inside an existing container of a pod.

~kubectl exec pod -- curl -s http://service_ip~

=The double dash (--) in the command signals the end of command options for kubectl.=

Using kubectl exec to test out a connection to the service by running curl in one of the pods
[[./pictures/kubernetes/40.png]]

这里是随机转发到不同的pod中
If you want all requests made by a certain client to be redirected to the same pod every time, you can =set the service’s sessionAffinity property to ClientIP=
#+begin_src yaml
apiVersion: v1
kind: Service
spec:
  sessionAffinity: ClientIP
  ...
#+end_src
This makes the service proxy redirect all requests originating from the same client IP to the same pod.

Kubernetes supports only two types of service session affinity: None and ClientIP.
You may be surprised it doesn’t have a cookie-based session affinity option, but you need to understand that Kubernetes services don’t operate at the HTTP level.
Services deal with TCP and UDP packets and don’t care about the payload they carry.
Because cookies are a construct of the HTTP protocol, services don’t know about them, which explains why session affinity cannot be based on cookies.

Using a single, multi-port service exposes all the service’s ports through a single cluster IP.
When creating a service with multiple ports, you must specify a name for each port.
#+begin_src yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia  
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  selector:
    app: kubia
#+end_src

=Using named ports=
#+begin_src yaml
kind: Pod
spec:
  containers:
  - name: kubia
    ports:
    - name: http
      containerPort: 8080
    - name: https
      containerPort; 8443
#+end_src

Referring to named ports in a service
#+begin_src yaml
apiVersion: v1
kind: Service
spec:
  ports:
  - name: http
    port: 80
    targetPort: http
  - name: https
    port: 443
    targetPort: https  
#+end_src

what if you later decide you’d like to move that to port 80?
=If you’re using named ports, all you need to do is change the port number in the pod spec (while keeping the port’s name unchanged).=
=As you spin up pods with the new ports, client connections will be forwarded to the appropriate port numbers, depending on the pod receiving the connection (port 8080 on old pods and port 80 on the new ones).=
** Discovering services
By creating a service, you now have a single and stable IP address and port that you can hit to access your pods.
=This address will remain unchanged throughout the whole lifetime of the service.=

But how do the client pods know the IP and port of a service?
=Kubernetes also provides ways for client pods to discover a service’s IP and port.=
=1.When a pod is started, Kubernetes initializes a set of environment variables pointing to each service that exists at that moment.=
~kubectl delete po --all~
~kubectl exec xxxpod env~
KUBIA_SERVICE_HOST and the KUBIA_SERVICE_PORT environment variables

when you have a frontend pod that requires the use of a backend database server pod, you can expose the backend pod through a service called backend-database and then have the frontend pod look up its IP address and port through the environment variables BACKEND_DATABASE_SERVICE_HOST and BACKEND_DATABASE_SERVICE_PORT.

=2.Discovering services through DNS=
~kubectl get pods -n kube-system~
kube-dns
=As the name suggests, the pod runs a DNS server, which all other pods running in the cluster are automatically configured to use (Kubernetes does that by modifying each container’s /etc/resolv.conf file).=
=Each service gets a DNS entry in the internal DNS server, and client pods that know the name of the service can access it through its fully qualified domain name (FQDN) instead of resorting to environment variables.=
  backend-database.default.svc.cluster.local
  You can omit the svc.cluster.local suffix and even the namespace, when the frontend pod is in the same namespace as the database pod.

~kubectl exec -it kubia-3inly bash~
~$curl http://kubia.default.svc.cluster.local~
~$curl http://kubia.default~
~$curl http://kubia~

You can omit the namespace and the svc.cluster.local suffix because of how the DNS resolver inside each pod’s container is configured.
~cat /etc/resolv.conf~
search default.svc.cluster.local svc.cluster.local cluster.local ...

=curl-ing the service works, but pinging it doesn’t. That’s because the service’s cluster IP is a virtual IP, and only has meaning when combined with the service port.=
** =Connecting to services living outside the cluster=
service <-> endpoint <-> inner pods
~kubectl describe svc kubia~
The service’s pod selector is used to create the list of endpoints.
The list of pod IPs and ports that represent the endpoints of this service
An Endpoints resource (yes, plural) is a list of IP addresses and ports exposing a service.
The Endpoints resource is like any other Kubernetes resource, so you can display its basic info with kubectl get
~kubectl get endpoints kubia~
=the pod selector is defined in the service spec, is used to build a list of IPs and ports, which is then stored in the Endpoints resource.=
** =Manually configuring service endpoints=
If you create a service without a pod selector, Kubernetes won’t even create the Endpoints resource (after all, without a selector, it can’t know which pods to include in the service).

create a service without a selector
#+begin_src yaml
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  ports:
  - port: 80  
#+end_src
没有selector就不会创建endpoints

=Creating an Endpoints resource for a service without a selector=
Endpoints are a separate resource and not an attribute of a service.
Because you created the service without a selector, the corresponding Endpoints resource hasn’t been created automatically,
#+begin_src yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: external-service
subsets:
  - addresses:
    - ip: 11.11.11.11
    - ip: 22.22.22.22
    ports:
    - port: 80  
#+end_src

The Endpoints object needs to have the same name as the service and contain the list of target IP addresses and ports for the service.
=After both the Service and the Endpoints resource are posted to the server, the service is ready to be used like any regular service with a pod selector.=

[[./pictures/kubernetes/41.png]]
=Containers created after the service is created will include the environment variables for the service, and all connections to its IP:port pair will be load balanced between the service’s endpoints.=
=If you later decide to migrate the external service to pods running inside Kubernetes, you can add a selector to the service, thereby making its Endpoints managed automatically.=
=The same is also true in reverse—by removing the selector from a Service, Kubernetes stops updating its Endpoints.=
This means a service IP address can remain constant while the actual implementation of the service is changed.？？？这句没懂
** Creating an alias for an external service
Instead of exposing an external service by manually configuring the service’s Endpoints,
a simpler method allows you to refer to an external service by its fully qualified domain name (FQDN).

#+begin_src yaml
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName
  externalName: someapi.somecompany.com
  ports:
  - port: 80  
#+end_src

After the service is created, pods can connect to the external service through the external-service.default.svc.cluster.local domain name (or even external-service) instead of using the service’s actual FQDN.
更灵活的后续切换：你可以稍后：
  修改 externalName 来指向另一个外部服务，或
  把 type 改成 ClusterIP，然后用 selector 或手动方式将其改为指向 Kubernetes 内部的 Pod。
这样，客户端不需要任何改动，服务提供者却可以随时切换服务实现或位置。

=ExternalName services are implemented solely at the DNS level—a simple CNAME DNS record is created for the service.=
=A CNAME record points to a fully qualified domain name instead of a numeric IP address.=
=Therefore, clients connecting to the service will connect to the external service directly, bypassing the service proxy completely=
因此，连接到该 Service 的客户端会直接连接到外部服务，完全绕过了 Kubernetes 的服务代理（如 kube-proxy）。
即不会走 ClusterIP、iptables、或 kube-proxy 中的 NAT 转发机制。
正因为如此，这类服务甚至不会分配一个 Cluster IP地址。
它没有虚拟 IP，也无法通过常规 Service 的方式进行负载均衡或流量拦截。
** Exposing services to external clients
[[./pictures/kubernetes/42.png]]

1.setting the service type to =NodePort=
  For a NodePort service, each cluster node opens a port on the node itself (hence the name) and redirects traffic received on that port to the underlying service.
2.setting the service to =LoadBalancer=
  This makes the service accessible through a dedicated load balancer, provisioned from the cloud infrastructure Kubernetes is running on.
3.Creating an =Ingress resource=, a radically different mechanism for exposing multiple services through a single IP address
  operates at the HTTP level (network layer 7) and can thus offer more features than layer 4 services can.

A NodePort service definition: kubia-svc-nodeport.yaml  
#+begin_src yaml
apiVersion: v1
kind: Service
metadata:
  nam: kubia-nodeport
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30123
selector:
  app: kubia
#+end_src

=Specifying the port isn’t mandatory; Kubernetes will choose a random port if you omit it.=

~kubectl get svc kubia-nodeport~

[[./pictures/kubernetes/43.png]]

when using GKE, need to configure the Google Cloud Platform’s firewalls to allow external connections to your nodes on that port.
~gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123~
** Using JSONPATH to get the IPs of all your nodes
can find the IP in the JSON or YAML descriptors of the nodes.

instead of sifting through the relatively large JSON, you can tell kubectl to print out only the node IP instead of the whole service definition

~kubectl get nodes -o json-path='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'~
JSONPath is basically XPath for JSON.
一种查询方式

http://kubernetes.io/docs/user-guide/jsonpath

=When using Minikube, you can easily access your NodePort services through your browser by running=
~minikube service <service-name> [-n <namespace>].~

But if you only point your clients to the first node, when that node fails, your clients can’t access the service anymore.
That’s why it makes sense to put a load balancer in front of the nodes to make sure you’re spreading requests across all healthy nodes and never sending them to a node that’s offline at that moment.
** Exposing a service through an external load balancer
a LoadBalancer service is an extension of a NodePort service.

Creating a LoadBalancer service
A LoadBalancer-type service: kubia-svc-loadbalancer.yaml
#+begin_src yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: kubia-loadbalancer
  spec:
    type: LoadBalancer
    ports:
    - port: 80
      targetPort: 8080
    selector:
      app: kubia
#+end_src

Connecting to the service through the load balancer
~kubectl get svc kubia-loadbalancer~

the browser will hit the exact same pod every time
The browser is using keep-alive connections and sends all its requests through a single connection, whereas curl opens a new connection every time.
Even if session affinity is set to None, users will always hit the same pod (until the connection is closed).

a LoadBalancer-type service is a NodePort service with an additional infrastructure-provided load balancer.
** Understanding the peculiarities of external connections
您可以通过配置服务仅将外部流量重定向到运行在接收连接的节点上的Pod来防止此额外的跳转。

#+begin_src yaml
spec:
  externalTrafficPolicy: Local
#+end_src

If a service definition includes this setting and an external connection is opened through the service’s node port, the service proxy will choose a locally running pod.
If no local pods exist, the connection will hang (it won’t be forwarded to a random global pod, the way connections are when not using the annotation).

A Service using the Local external traffic policy may lead to uneven load distribution across pods.
[[./pictures/kubernetes/44.png]]

=Usually, when clients inside the cluster connect to a service, the pods backing the service can obtain the client’s IP address.=
=But when the connection is received through a node port, the packets’ source IP is changed, because Source Network Address Translation (SNAT) is performed on the packets.=
The backing pod can’t see the actual client’s IP, which may be a problem for some applications that need to know the client’s IP.
In the case of a web server, for example, this means the access log won’t show the browser’s IP.

=The Local external traffic policy described in the previous section affects the preservation of the client’s IP, because there’s no additional hop between the node receiving the connection and the node hosting the target pod (SNAT isn’t performed).=
** Exposing services externally through an Ingress resource
exposing a service to clients outside the cluster:
  =1.NodePort=
  =2.LoadBalancer=
  =3.an Ingress resource=

Understanding why Ingresses are needed
=One important reason is that each LoadBalancer service requires its own load balancer with its own public IP address, whereas an Ingress only requires one, even when providing access to dozens of services.=

  [[./pictures/kubernetes/45.png]]
  Multiple services can be exposed through a single Ingress.

  =Ingresses operate at the application layer of the network stack (HTTP) and can provide features such as cookie-based session affinity and the like, which services can’t.=

=to make Ingress resources work, an Ingress controller needs to be running in the cluster.=
Different Kubernetes environments use different implementations of the controller, but several don’t provide a default controller at all.
  For example, Google Kubernetes Engine uses Google Cloud Platform’s own HTTP load-balancing features to provide the Ingress functionality.
  Minikube didn’t provide a controller out of the box, but it now includes an add-on that can be enabled to let you try out the Ingress functionality.
    Enabling the Ingress add-on in Minikube
    ~minikube addons list~
    ~minikube addons enable ingress~
      This should have spun up an Ingress controller as another pod.
    ~kubectl get po --all-namespace~
      (base) ➜  kubernetes git:(main) ✗ kubectl get pods --all-namespaces
      NAMESPACE       NAME                                        READY   STATUS      RESTARTS        AGE
      ingress-nginx   ingress-nginx-admission-create-dvpwc        0/1     Completed   0               51s
      ingress-nginx   ingress-nginx-admission-patch-wwjmd         0/1     Completed   0               51s
      ingress-nginx   ingress-nginx-controller-56d7c84fd4-pvxzm   0/1     Running     0               51s
      kube-system     coredns-668d6bf9bc-lbr5v                    1/1     Running     2 (4m12s ago)   11d
      kube-system     etcd-minikube                               1/1     Running     2 (4m12s ago)   11d
      kube-system     kube-apiserver-minikube                     1/1     Running     2 (4m12s ago)   11d
      kube-system     kube-controller-manager-minikube            1/1     Running     2 (4m12s ago)   11d
      kube-system     kube-proxy-rw6ld                            1/1     Running     2 (4m12s ago)   11d
      kube-system     kube-scheduler-minikube                     1/1     Running     2 (4m12s ago)   11d
      kube-system     storage-provisioner                         1/1     Running     4 (4m2s ago)    11d
    The name suggests that Nginx (an open-source HTTP server and reverse proxy) is used to provide the Ingress functionality.

=creating an Ingress resource=
An Ingress resource definition: kubia-ingress.yaml
#+begin_src yaml
apiVersion: Ingress
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com
    http:
      path:
      - paths: /
        backend:
          serviceName: kubia-nodeport
          servicePort: 80
#+end_src

Accessing the service through the Ingress
To access your service through http://kubia.example.com, =you’ll need to make sure the domain name resolves to the IP of the Ingress controller.=
~kubectl get ingress~

=Once you know the IP, you can then either configure your DNS servers to resolve kubia.example.com to that IP or you can add the following line to /etc/hosts=
~192.168.99.100    kubia.example.com~

Understanding how Ingresses work
[[./pictures/kubernetes/46.png]]

The client first performed a DNS lookup of kubia.example.com, and the DNS server (or the local operating system) returned the IP of the Ingress controller.
The client then sent an HTTP request to the Ingress controller and specified kubia.example.com in the Host header.
From that header, the controller determined which service the client is trying to access, looked up the pod IPs through the Endpoints object associated with the service, and forwarded the client’s request to one of the pods.
** Exposing multiple services through the same Ingress
An Ingress can map multiple hosts and paths to multiple services

Mapping different services to different paths of the same host
#+begin_src yaml
  ...
    - host: kubia.example.com
      http:
        paths:
        - path: /kubia
          backend:
            serviceName: kubia
            servicePort: 80
         - path: /bar
           backend:
             serviceName: bar
             servicePort: 80
#+end_src

Mapping different services to different hosts
#+begin_src yaml
  spec:
    rules:
    - host: foo.example.com
      http:
        paths:
        - path: /
          backend:
            serviceName: foo
            servicePort: 80
    - host: bar.example.com
      http:
        paths:
        - path: /
          backend:
            serviceName: bar
            servicePort: 80
#+end_src

=DNS needs to point both the foo.example.com and the bar.example.com domain names to the Ingress controller’s IP address.=
** Configuring Ingress to handle TLS traffic
Creating a TLS certificate for the Ingress

When a client opens a TLS connection to an Ingress controller, the controller terminates the TLS connection.
The communication between the client and the controller is encrypted, whereas the communication between the controller and the backend pod isn’t.

=To enable the controller to do that, you need to attach a certificate and a private key to the Ingress.=
=The two need to be stored in a Kubernetes resource called a Secret=
~openssl genrsa -out tls.key 2048~
~openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj~

Then you create the Secret from the two files like this:
~kubectl create secret tls tls-sercet -- cert=tls.cert --key=tls.key~

Signing certificates through the CertificateSigningRequest resource
=Instead of signing the certificate ourselves, you can get the certificate signed by creating a CertificateSigningRequest (CSR) resource.=
Users or their applications can create a regular certificate request, put it into a CSR, and then either a human operator or an automated process can approve the request like this:
~kubectl certificate approve <name of the CSR>~

The signed certificate can then be retrieved from the CSR’s status.certificate field.

=Note that a certificate signer component must be running in the cluster;=
otherwise creating CertificateSigningRequest and approving or denying them won’t have any effect.
=The private key and the certificate are now stored in the Secret called tls-secret.=

Ingress handling TLS traffic: kubia-ingress-tls.yaml
#+begin_src yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  tls:
  - hosts:
    - kubia.example.com
    sercetName: tls-secret
  rules:
  - host: kubia.example.com
  http:
    paths:
    - path: /
      backend:
        serviceName: kubia-nodeport
        servicePort: 80  
#+end_src

=Instead of deleting the Ingress and re-creating it from the new file, you can invoke kubectl apply -f kubia-ingress-tls.yaml, which updates the Ingress resource with what’s specified in the file.=

~curl -k -v https://kubia.example.com/kubia~

Support for Ingress features varies between the different Ingress controller implementations, so check the implementation-specific documentation to see what’s supported.

Ingresses are a relatively new Kubernetes feature, so you can expect to see many improvements and new features in the future.
Although they currently support only L7 (HTTP/HTTPS) load balancing, support for L4 load balancing is also planned.
** Signaling when a pod is ready to accept connections
You’ve already learned that pods are included as endpoints of a service if their labels match the service’s pod selector.
[[*=Connecting to services living outside the cluster=]]

